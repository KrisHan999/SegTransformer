model 1: 
	loss: focal loss and dice loss
	loss term: attention map from transformer decoder and output.
	output: output with air and issue
	
	issue: loss is not stable, mainly because the focal loss for large ROI overwhelme small ROI. Loss doesn't decrement but increment after certain epoches. Also, the attention map is bad as more training epoches.
model 2:
	loss: dice loss
	issue: Almost same except epoch loss is stable for removing focal loss. But the loss still increment and attention map is bad.
model 3:
	loss: dice loss
	structure: add avgpool with embedding at deepest layer
	benefit: loss is stable, no jump increament and decreament
	issue: attention map loss increase. Also all validation loss
	solution: The final segmentation may need pixel level knowledge, combine pixel feature with output of transformer before it is sent to decoder.
model 4:
	loss: dice loss
	structure: transformer_out = pixel_feature + transformer_out
	benefit: attention_map_loss is less the model 3
	issue: training dice loss is higher than model 4, which means the attention output in some sense does harm to segmentation. attention_map_loss doesn't decrease during training. All validation loss increases during training.
model 5:
	loss: dice loss
	structure: remove unet decoder, concatenate the output of transformer with enc_out before it is sent to the next transformer. Loss is only calculated based on the attention output, use batch normalization.
	benefit: retain the high level knowledge of structure by concatenation. 
	issue: mask is not uniform and val loss might be a issue.
model 6:
	loss: dice loss
	structure: compared to model 5, add a output branch to the end. The input is the combination of enc_out_1 and attemtion_map, wishing the convolution layer could summarize the feature.
	issue: it appears that convolution will do hard to all attention_map and it doesn't like attention map as input. 
Schedule:
	1. Transformer decoder: tgt_out = input + tgt_out. Add pixel level feature to the output out Transformer, because segmentation may need that feature to behavior better.
	2. Concate the input of transformer with the output of transformer to enforce more structure feature.
	3. Deepest layer: Add deformable self-attention mechanism to generate structure knowledge before it is sent to transformer.
